{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:cd:1: no such file or directory: ../src\r\n"
     ]
    }
   ],
   "source": [
    "!cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from src.utils import plots_dir, clusters_dir, features_dir\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plots\n",
    "font = {\n",
    "    'size': 16\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Experiment\n",
    "group = \"combined\"  # \"controlled\" or \"free\" or \"combined\"\n",
    "pos = \"adjectives\"  # \"nouns\" or \"adjectives\"\n",
    "\n",
    "colors = {\n",
    "    \"controlled\": \"#97121f\",\n",
    "    \"free\": \"#1271d1\",\n",
    "    \"combined\": \"#0b4539\"\n",
    "}\n",
    "\n",
    "# Embeddings\n",
    "embeddings_model = \"glove\"  # \"bert\" or \"glove\"\n",
    "\n",
    "# Clustering\n",
    "if group == \"controlled\" and pos == \"nouns\":\n",
    "    N_CLUSTERS = 9\n",
    "if group == \"controlled\" and pos == \"adjectives\":\n",
    "    N_CLUSTERS = 10\n",
    "if group == \"free\" and pos == \"nouns\":\n",
    "    N_CLUSTERS = 12\n",
    "if group == \"free\" and pos == \"adjectives\":\n",
    "    N_CLUSTERS = 11\n",
    "if group == \"combined\" and pos == \"nouns\":\n",
    "    N_CLUSTERS = 17\n",
    "if group == \"combined\" and pos == \"adjectives\":\n",
    "    N_CLUSTERS = 19\n",
    "\n",
    "# Base output paths\n",
    "base_filename = f\"{group}_{pos}_2_{embeddings_model}_agglo\"\n",
    "plots_path = plots_dir() / base_filename\n",
    "plots_path = str(plots_path)\n",
    "clusters_path = clusters_dir() / base_filename\n",
    "clusters_path = str(clusters_path) + f\"_n{N_CLUSTERS}\"\n",
    "features_path = features_dir() / \"linguistic\" / base_filename\n",
    "features_path = str(features_path) + f\"_n{N_CLUSTERS}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 121,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def get_concept_id(index):\n",
    "    if group == \"controlled\" and pos == \"nouns\":\n",
    "        return \"ConstNoun\" + str(index + 1)\n",
    "    if group == \"controlled\" and pos == \"adjectives\":\n",
    "        return \"ConstAdj\" + str(index + 1)\n",
    "    if group == \"free\" and pos == \"nouns\":\n",
    "        return \"FreeNoun\" + str(index + 1)\n",
    "    if group == \"free\" and pos == \"adjectives\":\n",
    "        return \"FreeAdj\" + str(index + 1)\n",
    "    if group == \"combined\" and pos == \"nouns\":\n",
    "        return \"CombNoun\" + str(index + 1)\n",
    "    if group == \"combined\" and pos == \"adjectives\":\n",
    "        return \"CombAdj\" + str(index + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load descriptors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "(80,\n ['first',\n  'rhythmic',\n  'excited',\n  'much',\n  'other',\n  'anxious',\n  'different',\n  'constant',\n  'more',\n  'like',\n  'sweaty',\n  'hard',\n  'stuck',\n  'slow',\n  'slight',\n  'little',\n  'normal',\n  'pleasant',\n  'sticky',\n  'uncomfortable',\n  'strong',\n  'fast',\n  'fun',\n  'funny',\n  'happy',\n  'nice',\n  'wrong',\n  'rough',\n  'light',\n  'electrical',\n  'small',\n  'tiny',\n  'heavy',\n  'smooth',\n  'textured',\n  'whole',\n  'aggressive',\n  'good',\n  'long',\n  'annoying',\n  'most',\n  'same',\n  'aware',\n  'alert',\n  'big',\n  'last',\n  'least',\n  'bad',\n  'soft',\n  'few',\n  'easy',\n  'deep',\n  'gentle',\n  'subtle',\n  'electric',\n  'low',\n  'slippery',\n  'intense',\n  'wet',\n  'actual',\n  'second',\n  'super',\n  'quick',\n  'calming',\n  'high',\n  'angry',\n  'short',\n  'weird',\n  'natural',\n  'similar',\n  'sure',\n  'cold',\n  'continuous',\n  'satisfied',\n  'specific',\n  'okay',\n  'frequent',\n  'quiet',\n  'difficult',\n  'calm'])"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils import descriptors_dir\n",
    "\n",
    "descriptors = np.genfromtxt(descriptors_dir() / f\"{group}_{pos}.txt\", dtype=str)\n",
    "len(descriptors), list(descriptors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get static embeddings for descriptors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 300)\n"
     ]
    }
   ],
   "source": [
    "if embeddings_model == \"bert\":\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    embeddings = model.encode(descriptors, convert_to_numpy=True, show_progress_bar=True)\n",
    "else:  # \"glove\"\n",
    "    from src.glove import get_embeddings\n",
    "    embeddings_dict = get_embeddings(n_dim=300)\n",
    "    embeddings = np.array([embeddings_dict[word] for word in descriptors])\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute clusters and save"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "labels = AgglomerativeClustering(affinity=\"euclidean\",\n",
    "                                 n_clusters=N_CLUSTERS,\n",
    "                                 linkage=\"ward\"\n",
    "                                 ).fit_predict(embeddings)\n",
    "\n",
    "concept_to_desc = {} # k: concept_idx, v: list of descriptors\n",
    "desc_to_concept = {} # k: desc_idx, v: concept_idx\n",
    "\n",
    "for concept_idx in range(0, N_CLUSTERS):\n",
    "    desc_indices = list(np.argwhere(labels == concept_idx))\n",
    "    desc_indices = [desc_idx[0] for desc_idx in desc_indices]\n",
    "    concept_to_desc[concept_idx] = [descriptors[desc_idx] for desc_idx in desc_indices]\n",
    "\n",
    "    for desc_idx in desc_indices:\n",
    "        desc_to_concept[desc_idx] = concept_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "# Save as JSON\n",
    "# path = clusters_path + \".json\"\n",
    "# with open(path, \"w\") as fp:\n",
    "#     json.dump(concept_to_desc, fp)\n",
    "#\n",
    "# print(f\"See clusters at {path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "path = clusters_dir() / f\"{group}_{pos}.csv\"\n",
    "concepts = []\n",
    "for i, (key, value) in enumerate(concept_to_desc.items()):\n",
    "    desc_elements = \" \".join(value)\n",
    "    concept_id = get_concept_id(i)\n",
    "    row = [concept_id, desc_elements]\n",
    "    concepts.append(row)\n",
    "header = [ [ \"Concept\", \"Descriptors\" ] ]\n",
    "concepts = np.vstack((header, concepts))\n",
    "np.savetxt(path, concepts, delimiter=\",\", fmt=\"%s\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identify cluster centers and save\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "concept_centers = []\n",
    "for concept_idx in range(0, N_CLUSTERS):\n",
    "    desc_indices = list(np.argwhere(labels == concept_idx))\n",
    "    desc_indices = [desc_idx[0] for desc_idx in desc_indices]\n",
    "    desc_embeddings = embeddings[desc_indices] # get embeddings for descriptors in concept\n",
    "    distance_matrix = euclidean_distances(desc_embeddings, desc_embeddings) # compute distances\n",
    "    summed_distance_matrix = np.sum(distance_matrix, axis=1) # sum up distances per descriptor\n",
    "    min_idx = np.argmin(summed_distance_matrix) # find min overall distance\n",
    "    concept_center_idx = desc_indices[min_idx]\n",
    "    concept_centers.append(descriptors[concept_center_idx])\n",
    "\n",
    "# np.savetxt(clusters_path + \"_centers.csv\", concept_centers, delimiter=\",\", fmt=\"%s\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From clusters to concept matrix\n",
    "\n",
    "1. Replace words in normalized pattern descriptions with the concept they are associated with."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "from src.data import get_merged_norm_descriptions\n",
    "\n",
    "merged_norm_descriptions = get_merged_norm_descriptions(group) # 32 patterns\n",
    "concepts = []\n",
    "\n",
    "for description in merged_norm_descriptions:\n",
    "    concept_names = []\n",
    "    for word in description.split():\n",
    "        word_indices = np.where(np.array(descriptors) == word)[0]\n",
    "        if len(word_indices) > 0:  # word is in descriptors\n",
    "            word_idx = word_indices[0]\n",
    "            concept_idx = desc_to_concept[word_idx]\n",
    "            concept_name = f\"concept_{concept_idx}\"\n",
    "            concept_names.append(concept_name)\n",
    "\n",
    "    concepts.append(\" \".join(concept_names))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF\n",
    "\n",
    "1. Compute TF-IDF feature matrix on the new pattern descriptions that include concept ids instead of words.\n",
    "2. Save feature matrix to disk."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(concepts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "ids = [ [id] for id in range(1, 33) ]\n",
    "features = X_tfidf.todense()\n",
    "features = np.hstack((ids, features))\n",
    "columns = [\"id\"]\n",
    "for feature_name in vectorizer.get_feature_names_out():\n",
    "    columns.append(feature_name)\n",
    "df = pd.DataFrame(features, columns=columns)\n",
    "df = df.round(4)\n",
    "df = df.astype({ \"id\": int })\n",
    "# df.to_csv(features_path + \"_tfidf.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "1. Count occurrences of concepts.\n",
    "2. Save feature matrix to disk."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_count = vectorizer.fit_transform(concepts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "ids = [ [id] for id in range(1, 33) ]\n",
    "features = X_count.todense()\n",
    "features = np.hstack((ids, features))\n",
    "columns = [\"id\"]\n",
    "for feature_name in vectorizer.get_feature_names_out():\n",
    "    columns.append(feature_name)\n",
    "df = pd.DataFrame(features, columns=columns)\n",
    "df = df.astype({ \"id\": int })\n",
    "# df.to_csv(features_path + \"_count.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "933bef1053b175de4736f56775eabf2339dd22c815971fabe7d440e8e2c9504a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}